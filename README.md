# Deep Learning Course Projects

This repository contains the coursework for a Deep Learning course, including several assignments and a final project. Each assignment explores different deep learning concepts, ranging from basic forward propagation to more advanced topics like convolutional neural networks (CNNs) and recurrent neural networks (RNNs).

## Table of Contents

1. [Overview](#overview)
2. [Project Structure](#project-structure)
3. [HW1 - Forward Propagation](#hw1---forward-propagation)
4. [HW2 - Backpropagation and Gradients](#hw2---backpropagation-and-gradients)
5. [HW3 - Learning and Basic Architectures](#hw3---learning-and-basic-architectures)
6. [HW4 - Regularization Techniques](#hw4---regularization-techniques)
7. [HW5 - Convolutional Neural Networks (CNNs)](#hw5---convolutional-neural-networks-cnns)
8. [Final Project - Recurrent Neural Network (RNN) for Word Generation](#final-project---recurrent-neural-network-rnn-for-word-generation)


## Overview

This repository includes a series of assignments designed to build foundational skills in deep learning. Each assignment focuses on a different aspect of deep learning, such as forward and backward propagation, learning architectures, and regularization techniques. The final project involves implementing a character-level recurrent neural network (RNN) for word generation.

## Project Structure

The repository is organized into the following directories:

- `HW1/` - Forward Propagation.
- `HW2/` - Backpropagation and Gradients.
- `HW3/` - Learning and Basic Architectures.
- `HW4/` - Regularization Techniques.
- `HW5/` - Convolutional Neural Networks (CNNs).
- `Final/` - Recurrent Neural Network (RNN) for Word Generation (Final Project).

## HW1 - Forward Propagation

**Objective**: Implement the forward propagation for a simple neural network, starting with basic operations like matrix multiplications and activation functions.

**Key Components**:
- **Fully Connected Layer**: Implementation of forward propagation through a fully connected layer.
- **Activation Functions**: Implement various activation functions (ReLU, Sigmoid, Tanh) and observe their effects.
- **Simple Network Assembly**: Construct a simple network and propagate data through it.

## HW2 - Backpropagation and Gradients

**Objective**: Extend the implementation to include backward propagation, calculating gradients for different layers and updating weights.

**Key Components**:
- **Backward Method Implementation**: Implement backpropagation for fully connected layers and activation functions.
- **Gradient Calculation**: Derive and implement gradient calculations for each layer.
- **Objective Functions**: Implement squared error, log loss, and cross-entropy objective functions.

## HW3 - Learning and Basic Architectures

**Objective**: Explore the process of learning through gradient descent and implement basic neural network architectures.

**Key Components**:
- **Gradient Descent Visualization**: Visualize the gradient descent process for simple functions.
- **Linear and Logistic Regression**: Implement and train linear and logistic regression models using neural network principles.

## HW4 - Regularization Techniques

**Objective**: Implement and explore the effects of various regularization techniques on the learning process.

**Key Components**:
- **L2 Regularization**: Implement and apply L2 regularization in neural networks.
- **Dropout**: Explore the effects of dropout on network performance.
- **Weight Decay**: Implement weight decay as a regularization technique and analyze its impact.

## HW5 - Convolutional Neural Networks (CNNs)

**Objective**: Implement convolutional layers and pooling operations, culminating in a simple CNN for image classification.

**Key Components**:
- **Convolutional Layer**: Implement forward propagation through convolutional layers.
- **Pooling Layers**: Implement max-pooling and average-pooling layers.
- **CNN Architecture**: Build and train a CNN for image classification on synthetic and real datasets.

## Final Project - Recurrent Neural Network (RNN) for Word Generation

**Objective**: Implement a character-level RNN to generate word sequences, demonstrating the application of RNNs in language processing tasks.

**Key Components**:
- **RNN Implementation**: Develop a one-layer RNN for word generation.
- **Hyperparameter Tuning**: Optimize the RNN by adjusting learning rate, sequence length, and other parameters.
- **Performance Evaluation**: Evaluate the RNNâ€™s ability to generate coherent word sequences.
